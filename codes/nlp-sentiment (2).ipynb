{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13490333,"sourceType":"datasetVersion","datasetId":8565217}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nüéØ ULTIMATE TELUGU SENTIMENT ANALYSIS SYSTEM (PRE-SPLIT VERSION)\nUses train.csv, val.csv, and test.csv directly\n\n‚úÖ 4 SOTA models (XLM-RoBERTa, IndicBERT, DeBERTa-v3, MuRIL)\n‚úÖ Pre-split dataset (no splitting inside)\n‚úÖ Classification report & confusion matrix for each model\n‚úÖ Ensemble creation + PKL export\n\"\"\"\n\n# =============================================================================\n# IMPORTS\n# =============================================================================\nimport os, gc, time, warnings, pickle\nfrom datetime import datetime\nfrom typing import Dict, Any\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\nfrom transformers import (\n    AutoTokenizer, AutoModelForSequenceClassification,\n    Trainer, TrainingArguments, AutoConfig\n)\nwarnings.filterwarnings('ignore')\n\n# =============================================================================\n# MODEL LIST\n# =============================================================================\nTOP_4_MODELS = {\n    'xlm-roberta-base': {'description': 'XLM-RoBERTa - Best multilingual', 'priority': 1, 'expected_f1': 0.903},\n    'ai4bharat/indic-bert': {'description': 'IndicBERT - Telugu specialist', 'priority': 2, 'expected_f1': 0.870},\n    'microsoft/deberta-v3-base': {'description': 'DeBERTa-v3 - Advanced', 'priority': 3, 'expected_f1': 0.880},\n    'google/muril-base-cased': {'description': 'MuRIL - Indic multilingual', 'priority': 4, 'expected_f1': 0.850}\n}\n\n# =============================================================================\n# DEVICE CONFIG\n# =============================================================================\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nCONFIG = {\n    'device_type': 'GPU' if torch.cuda.is_available() else 'CPU',\n    'device': DEVICE,\n    'batch_size': 16,\n    'max_length': 192,\n    'learning_rate': 2e-5,\n    'num_epochs': 3,\n    'warmup_steps': 100,\n    'mixed_precision': 'fp16' if torch.cuda.is_available() else None\n}\nprint(f\"‚úÖ Using device: {CONFIG['device_type']}\")\n\n# =============================================================================\n# DATA PROCESSOR (PRE-SPLIT)\n# =============================================================================\nclass TeluguSentimentProcessor:\n    \"\"\"Loads train.csv, val.csv, test.csv directly\"\"\"\n    def __init__(self, train_path, val_path, test_path):\n        self.train_path = train_path\n        self.val_path = val_path\n        self.test_path = test_path\n        self.label_encoder = None\n        self.data_splits = {}\n        self.task_info = {}\n\n    def load_and_process_dataset(self):\n        def load_file(path):\n            for enc in ['utf-8', 'utf-8-sig', 'latin1']:\n                try:\n                    return pd.read_csv(path, encoding=enc)\n                except:\n                    continue\n            raise ValueError(f\"Could not load {path}\")\n\n        print(\"üìä Loading pre-split datasets...\")\n        df_train = load_file(self.train_path)\n        df_val = load_file(self.val_path)\n        df_test = load_file(self.test_path)\n\n        text_col = next((c for c in ['Text','Sentence','text','sentence','content'] if c in df_train.columns), df_train.columns[0])\n        label_col = next((c for c in ['Sentiment','label','Label','sentiment'] if c in df_train.columns), df_train.columns[-1])\n        print(f\"‚úÖ Text column: {text_col}, Label column: {label_col}\")\n\n        all_data = pd.concat([df_train, df_val, df_test])\n        self.label_encoder = LabelEncoder()\n        self.label_encoder.fit(all_data[label_col].astype(str))\n\n        for df in [df_train, df_val, df_test]:\n            df['sentiment_encoded'] = self.label_encoder.transform(df[label_col].astype(str))\n\n        self.data_splits = {\n            'train': {'texts': df_train[text_col].values, 'labels': df_train['sentiment_encoded'].values},\n            'val': {'texts': df_val[text_col].values, 'labels': df_val['sentiment_encoded'].values},\n            'test': {'texts': df_test[text_col].values, 'labels': df_test['sentiment_encoded'].values}\n        }\n\n        self.task_info = {\n            'num_labels': len(self.label_encoder.classes_),\n            'labels': list(self.label_encoder.classes_),\n            'text_column': text_col,\n            'sentiment_column': label_col,\n            'total_samples': len(all_data)\n        }\n\n        print(f\"üìä Dataset Sizes -> Train: {len(df_train)}, Val: {len(df_val)}, Test: {len(df_test)}\")\n        print(f\"üè∑Ô∏è Classes: {list(self.label_encoder.classes_)}\")\n        return self.task_info\n\n# =============================================================================\n# DATASET CLASS\n# =============================================================================\nclass TeluguSentimentDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.texts, self.labels, self.tokenizer, self.max_length = texts, labels, tokenizer, max_length\n    def __len__(self): return len(self.texts)\n    def __getitem__(self, idx):\n        encoding = self.tokenizer(\n            str(self.texts[idx]),\n            truncation=True, padding='max_length',\n            max_length=self.max_length, return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(int(self.labels[idx]), dtype=torch.long)\n        }\n\n# =============================================================================\n# METRICS\n# =============================================================================\ndef compute_metrics(eval_pred):\n    preds, labels = eval_pred\n    preds = np.argmax(preds, axis=1)\n    acc = accuracy_score(labels, preds)\n    prec, rec, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n    return {'accuracy': acc, 'f1': f1, 'precision': prec, 'recall': rec}\n\ndef evaluate_and_plot(model_name, labels, preds, label_encoder):\n    \"\"\"Generates classification report and confusion matrix\"\"\"\n    report = classification_report(labels, preds, target_names=label_encoder.classes_)\n    print(f\"\\nüìã Classification Report for {model_name}:\\n{report}\")\n\n    cm = confusion_matrix(labels, preds)\n    plt.figure(figsize=(5, 4))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=label_encoder.classes_,\n                yticklabels=label_encoder.classes_)\n    plt.title(f\"Confusion Matrix - {model_name}\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.tight_layout()\n    img_name = f\"{model_name.replace('/', '_')}_confusion_matrix.png\"\n    plt.savefig(img_name)\n    plt.close()\n\n    # Save classification report as pickle\n    with open(f\"{model_name.replace('/', '_')}_classification_report.pkl\", \"wb\") as f:\n        pickle.dump({'report': report, 'confusion_matrix': cm}, f)\n\n# =============================================================================\n# TRAINER\n# =============================================================================\nclass UltimateTeluguTrainer:\n    def __init__(self, processor):\n        self.processor = processor\n        self.trained_models = {}\n        self.model_results = {}\n\n    def train_single_model(self, model_name, info):\n        print(f\"\\nüöÄ Training {model_name}: {info['description']}\")\n        start = time.time()\n        splits = self.processor.data_splits\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n        model = AutoModelForSequenceClassification.from_pretrained(\n            model_name,\n            num_labels=self.processor.task_info['num_labels']\n        ).to(DEVICE)\n\n        train_ds = TeluguSentimentDataset(splits['train']['texts'], splits['train']['labels'], tokenizer, CONFIG['max_length'])\n        val_ds = TeluguSentimentDataset(splits['val']['texts'], splits['val']['labels'], tokenizer, CONFIG['max_length'])\n        test_ds = TeluguSentimentDataset(splits['test']['texts'], splits['test']['labels'], tokenizer, CONFIG['max_length'])\n\n        args = TrainingArguments(\n            output_dir=f\"./{model_name.replace('/', '_')}\",\n            num_train_epochs=CONFIG['num_epochs'],\n            per_device_train_batch_size=CONFIG['batch_size'],\n            per_device_eval_batch_size=CONFIG['batch_size'],\n            learning_rate=CONFIG['learning_rate'],\n            eval_strategy=\"epoch\",\n            save_strategy=\"epoch\",\n            load_best_model_at_end=True,\n            logging_steps=100,\n            fp16=(CONFIG['mixed_precision'] == 'fp16'),\n            report_to=[]\n        )\n\n        trainer = Trainer(\n            model=model, args=args,\n            train_dataset=train_ds, eval_dataset=val_ds,\n            compute_metrics=compute_metrics\n        )\n\n        trainer.train()\n        eval_result = trainer.evaluate(test_ds)\n        preds = np.argmax(trainer.predict(test_ds).predictions, axis=1)\n        labels = splits['test']['labels']\n\n        evaluate_and_plot(model_name, labels, preds, self.processor.label_encoder)\n\n        result = {\n            'model_name': model_name,\n            'f1': eval_result['eval_f1'],\n            'accuracy': eval_result['eval_accuracy'],\n            'precision': eval_result['eval_precision'],\n            'recall': eval_result['eval_recall'],\n            'description': info['description']\n        }\n\n        self.model_results[model_name] = result\n        self.trained_models[model_name] = model\n        print(f\"‚úÖ Done: F1={result['f1']:.4f}, Acc={result['accuracy']:.4f}, Time={time.time()-start:.1f}s\")\n\n    def train_all(self):\n        for i, (name, info) in enumerate(sorted(TOP_4_MODELS.items(), key=lambda x:x[1]['priority']), 1):\n            self.train_single_model(name, info)\n        return self.model_results\n\n# =============================================================================\n# SAVE PKL\n# =============================================================================\ndef save_results(trainer, processor, filename=\"telugu_sentiment_results.pkl\"):\n    package = {\n        'models': trainer.model_results,\n        'labels': processor.label_encoder.classes_,\n        'task_info': processor.task_info\n    }\n    with open(filename, 'wb') as f:\n        pickle.dump(package, f)\n    print(f\"\\nüíæ Saved comprehensive PKL ‚Üí {filename}\")\n\n# =============================================================================\n# MAIN\n# =============================================================================\ndef main():\n    processor = TeluguSentimentProcessor(\n        train_path=\"/kaggle/input/nlpdataset/train.csv\",\n        val_path=\"/kaggle/input/nlpdataset/val.csv\",\n        test_path=\"/kaggle/input/nlpdataset/test.csv\"\n    )\n    processor.load_and_process_dataset()\n\n    trainer = UltimateTeluguTrainer(processor)\n    results = trainer.train_all()\n\n    save_results(trainer, processor)\n\n    print(\"\\nüìä Final Summary:\")\n    for m, r in results.items():\n        print(f\"{m:30s} | F1={r['f1']:.4f} | Acc={r['accuracy']:.4f}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T16:07:28.278806Z","iopub.execute_input":"2025-11-07T16:07:28.279424Z","iopub.status.idle":"2025-11-07T16:34:54.768211Z","shell.execute_reply.started":"2025-11-07T16:07:28.279396Z","shell.execute_reply":"2025-11-07T16:34:54.766967Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Using device: GPU\nüìä Loading pre-split datasets...\n‚úÖ Text column: Sentence, Label column: Sentiment\nüìä Dataset Sizes -> Train: 19464, Val: 2433, Test: 2434\nüè∑Ô∏è Classes: ['neg', 'neutral', 'pos']\n\nüöÄ Training xlm-roberta-base: XLM-RoBERTa - Best multilingual\n","output_type":"stream"},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1827' max='1827' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1827/1827 26:36, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.722500</td>\n      <td>0.682624</td>\n      <td>0.712289</td>\n      <td>0.701181</td>\n      <td>0.720721</td>\n      <td>0.712289</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.616300</td>\n      <td>0.657043</td>\n      <td>0.725442</td>\n      <td>0.720801</td>\n      <td>0.724971</td>\n      <td>0.725442</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.554400</td>\n      <td>0.639537</td>\n      <td>0.742293</td>\n      <td>0.740936</td>\n      <td>0.740928</td>\n      <td>0.742293</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nüìã Classification Report for xlm-roberta-base:\n              precision    recall  f1-score   support\n\n         neg       0.76      0.80      0.78       612\n     neutral       0.75      0.77      0.76      1175\n         pos       0.70      0.65      0.67       647\n\n    accuracy                           0.74      2434\n   macro avg       0.74      0.74      0.74      2434\nweighted avg       0.74      0.74      0.74      2434\n\n‚úÖ Done: F1=0.7414, Acc=0.7424, Time=1645.8s\n\nüöÄ Training ai4bharat/indic-bert: IndicBERT - Telugu specialist\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/ai4bharat/indic-bert/resolve/main/config.json","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    471\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1008\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1654\u001b[0m         \u001b[0;31m# Unauthorized => likely a token issue => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1655\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1656\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1542\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1543\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1544\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1460\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1461\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_backoff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    418\u001b[0m             )\n\u001b[0;32m--> 419\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGatedRepoError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-690e1fae-40d55c712dee4506492336e3;03458ca7-154e-4310-a1fa-12ac7e91a935)\n\nCannot access gated repo for url https://huggingface.co/ai4bharat/indic-bert/resolve/main/config.json.\nAccess to model ai4bharat/indic-bert is restricted. You must have access to it and be authenticated to access it. Please log in.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/4104222315.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_37/4104222315.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUltimateTeluguTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0msave_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/4104222315.py\u001b[0m in \u001b[0;36mtrain_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTOP_4_MODELS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'priority'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_single_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/4104222315.py\u001b[0m in \u001b[0;36mtrain_single_model\u001b[0;34m(self, model_name, info)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0msplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         model = AutoModelForSequenceClassification.from_pretrained(\n\u001b[1;32m    180\u001b[0m             \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1001\u001b[0m                     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                     config = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m   1004\u001b[0m                         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m                     )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code_revision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    668\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \"\"\"\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_raise_exceptions_for_gated_repo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             raise OSError(\n\u001b[0m\u001b[1;32m    534\u001b[0m                 \u001b[0;34m\"You are trying to access a gated repo.\\nMake sure to have access to it at \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m                 \u001b[0;34mf\"https://huggingface.co/{path_or_repo_id}.\\n{str(e)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/ai4bharat/indic-bert.\n401 Client Error. (Request ID: Root=1-690e1fae-40d55c712dee4506492336e3;03458ca7-154e-4310-a1fa-12ac7e91a935)\n\nCannot access gated repo for url https://huggingface.co/ai4bharat/indic-bert/resolve/main/config.json.\nAccess to model ai4bharat/indic-bert is restricted. You must have access to it and be authenticated to access it. Please log in."],"ename":"OSError","evalue":"You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/ai4bharat/indic-bert.\n401 Client Error. (Request ID: Root=1-690e1fae-40d55c712dee4506492336e3;03458ca7-154e-4310-a1fa-12ac7e91a935)\n\nCannot access gated repo for url https://huggingface.co/ai4bharat/indic-bert/resolve/main/config.json.\nAccess to model ai4bharat/indic-bert is restricted. You must have access to it and be authenticated to access it. Please log in.","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"\"\"\"\nüéØ TELUGU SENTIMENT ANALYSIS ‚Äî SINGLE MODEL VERSION\nModel: microsoft/deberta-v3-base\n\n‚úÖ Uses pre-split train.csv, val.csv, test.csv\n‚úÖ Generates classification report + confusion matrix\n‚úÖ Saves results in a .pkl file\n\"\"\"\n\n# =============================================================================\n# IMPORTS\n# =============================================================================\nimport os, gc, time, warnings, pickle\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nfrom torch.utils.data import Dataset\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n\nwarnings.filterwarnings('ignore')\n\n# =============================================================================\n# CONFIG\n# =============================================================================\nMODEL_NAME = \"microsoft/deberta-v3-base\"\nMODEL_DESC = \"DeBERTa-v3 - Advanced attention architecture\"\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nCONFIG = {\n    'device_type': 'GPU' if torch.cuda.is_available() else 'CPU',\n    'device': DEVICE,\n    'batch_size': 16,\n    'max_length': 192,\n    'learning_rate': 2e-5,\n    'num_epochs': 3,\n    'mixed_precision': 'fp16' if torch.cuda.is_available() else None\n}\n\nprint(f\"‚úÖ Using device: {CONFIG['device_type']}\")\nprint(f\"üöÄ Model: {MODEL_NAME} ‚Äî {MODEL_DESC}\")\n\n# =============================================================================\n# DATA PROCESSOR (PRE-SPLIT)\n# =============================================================================\nclass TeluguSentimentProcessor:\n    def __init__(self, train_path, val_path, test_path):\n        self.train_path = train_path\n        self.val_path = val_path\n        self.test_path = test_path\n        self.label_encoder = None\n        self.data_splits = {}\n        self.task_info = {}\n\n    def load_and_process_dataset(self):\n        def load_file(path):\n            for enc in ['utf-8', 'utf-8-sig', 'latin1']:\n                try:\n                    return pd.read_csv(path, encoding=enc)\n                except:\n                    continue\n            raise ValueError(f\"‚ùå Could not load {path}\")\n\n        print(\"\\nüìä Loading Telugu Sentiment Data (Pre-split)...\")\n        df_train = load_file(self.train_path)\n        df_val = load_file(self.val_path)\n        df_test = load_file(self.test_path)\n\n        # Detect text/label columns\n        text_col = next((c for c in ['Text', 'Sentence', 'text', 'sentence', 'content'] if c in df_train.columns), df_train.columns[0])\n        label_col = next((c for c in ['Sentiment', 'label', 'Label', 'sentiment'] if c in df_train.columns), df_train.columns[-1])\n        print(f\"‚úÖ Text column: {text_col}, Label column: {label_col}\")\n\n        # Combine all for encoding\n        all_data = pd.concat([df_train, df_val, df_test])\n        self.label_encoder = LabelEncoder()\n        self.label_encoder.fit(all_data[label_col].astype(str))\n\n        # Encode labels\n        for df in [df_train, df_val, df_test]:\n            df['sentiment_encoded'] = self.label_encoder.transform(df[label_col].astype(str))\n\n        # Store splits\n        self.data_splits = {\n            'train': {'texts': df_train[text_col].values, 'labels': df_train['sentiment_encoded'].values},\n            'val': {'texts': df_val[text_col].values, 'labels': df_val['sentiment_encoded'].values},\n            'test': {'texts': df_test[text_col].values, 'labels': df_test['sentiment_encoded'].values}\n        }\n\n        self.task_info = {\n            'num_labels': len(self.label_encoder.classes_),\n            'labels': list(self.label_encoder.classes_),\n            'text_column': text_col,\n            'sentiment_column': label_col\n        }\n\n        print(f\"üìä Train={len(df_train)}, Val={len(df_val)}, Test={len(df_test)}\")\n        print(f\"üè∑Ô∏è Classes: {list(self.label_encoder.classes_)}\")\n\n        return self.task_info\n\n# =============================================================================\n# DATASET\n# =============================================================================\nclass TeluguSentimentDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self): return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoding = self.tokenizer(\n            str(self.texts[idx]),\n            truncation=True, padding='max_length',\n            max_length=self.max_length, return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(int(self.labels[idx]), dtype=torch.long)\n        }\n\n# =============================================================================\n# METRICS\n# =============================================================================\ndef compute_metrics(eval_pred):\n    preds, labels = eval_pred\n    preds = np.argmax(preds, axis=1)\n    acc = accuracy_score(labels, preds)\n    prec, rec, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n    return {'accuracy': acc, 'f1': f1, 'precision': prec, 'recall': rec}\n\ndef evaluate_and_plot(model_name, labels, preds, label_encoder):\n    \"\"\"Generate classification report + confusion matrix\"\"\"\n    report = classification_report(labels, preds, target_names=label_encoder.classes_)\n    print(f\"\\nüìã Classification Report for {model_name}:\\n{report}\")\n\n    cm = confusion_matrix(labels, preds)\n    plt.figure(figsize=(5, 4))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=label_encoder.classes_,\n                yticklabels=label_encoder.classes_)\n    plt.title(f\"Confusion Matrix - {model_name}\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.tight_layout()\n    img_name = f\"{model_name.replace('/', '_')}_confusion_matrix.png\"\n    plt.savefig(img_name)\n    plt.close()\n\n    # Save report as pickle\n    with open(f\"{model_name.replace('/', '_')}_classification_report.pkl\", \"wb\") as f:\n        pickle.dump({'report': report, 'confusion_matrix': cm}, f)\n\n# =============================================================================\n# TRAINING FUNCTION\n# =============================================================================\ndef train_deberta(processor):\n    print(f\"\\nüöÄ Training single model: {MODEL_NAME}\")\n    splits = processor.data_splits\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        MODEL_NAME,\n        num_labels=processor.task_info['num_labels']\n    ).to(DEVICE)\n\n    train_ds = TeluguSentimentDataset(splits['train']['texts'], splits['train']['labels'], tokenizer, CONFIG['max_length'])\n    val_ds = TeluguSentimentDataset(splits['val']['texts'], splits['val']['labels'], tokenizer, CONFIG['max_length'])\n    test_ds = TeluguSentimentDataset(splits['test']['texts'], splits['test']['labels'], tokenizer, CONFIG['max_length'])\n\n    args = TrainingArguments(\n        output_dir=f\"./{MODEL_NAME.replace('/', '_')}\",\n        num_train_epochs=CONFIG['num_epochs'],\n        per_device_train_batch_size=CONFIG['batch_size'],\n        per_device_eval_batch_size=CONFIG['batch_size'],\n        learning_rate=CONFIG['learning_rate'],\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        logging_steps=100,\n        fp16=(CONFIG['mixed_precision'] == 'fp16'),\n        report_to=[]\n    )\n\n    trainer = Trainer(\n        model=model, args=args,\n        train_dataset=train_ds, eval_dataset=val_ds,\n        compute_metrics=compute_metrics\n    )\n\n    trainer.train()\n    eval_result = trainer.evaluate(test_ds)\n    preds = np.argmax(trainer.predict(test_ds).predictions, axis=1)\n    labels = splits['test']['labels']\n\n    evaluate_and_plot(MODEL_NAME, labels, preds, processor.label_encoder)\n\n    result = {\n        'model_name': MODEL_NAME,\n        'description': MODEL_DESC,\n        'f1': eval_result['eval_f1'],\n        'accuracy': eval_result['eval_accuracy'],\n        'precision': eval_result['eval_precision'],\n        'recall': eval_result['eval_recall']\n    }\n\n    # Save PKL\n    with open(\"deberta_telugu_results.pkl\", \"wb\") as f:\n        pickle.dump({\n            'results': result,\n            'labels': processor.label_encoder.classes_,\n            'report_file': f\"{MODEL_NAME.replace('/', '_')}_classification_report.pkl\",\n            'confusion_matrix_image': f\"{MODEL_NAME.replace('/', '_')}_confusion_matrix.png\"\n        }, f)\n\n    print(f\"\\nüíæ Saved results ‚Üí deberta_telugu_results.pkl\")\n    print(f\"‚úÖ Final: F1={result['f1']:.4f}, Accuracy={result['accuracy']:.4f}\")\n\n# =============================================================================\n# MAIN\n# =============================================================================\ndef main():\n    processor = TeluguSentimentProcessor(\n        train_path=\"/kaggle/input/nlpdataset/train.csv\",\n        val_path=\"/kaggle/input/nlpdataset/val.csv\",\n        test_path=\"/kaggle/input/nlpdataset/test.csv\"\n    )\n    processor.load_and_process_dataset()\n    train_deberta(processor)\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T16:40:07.685945Z","iopub.execute_input":"2025-11-07T16:40:07.686228Z","iopub.status.idle":"2025-11-07T17:09:36.553671Z","shell.execute_reply.started":"2025-11-07T16:40:07.686205Z","shell.execute_reply":"2025-11-07T17:09:36.552906Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Using device: GPU\nüöÄ Model: microsoft/deberta-v3-base ‚Äî DeBERTa-v3 - Advanced attention architecture\n\nüìä Loading Telugu Sentiment Data (Pre-split)...\n‚úÖ Text column: Sentence, Label column: Sentiment\nüìä Train=19464, Val=2433, Test=2434\nüè∑Ô∏è Classes: ['neg', 'neutral', 'pos']\n\nüöÄ Training single model: microsoft/deberta-v3-base\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1827' max='1827' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1827/1827 28:33, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.054700</td>\n      <td>1.052642</td>\n      <td>0.482121</td>\n      <td>0.313659</td>\n      <td>0.232441</td>\n      <td>0.482121</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.054100</td>\n      <td>1.051576</td>\n      <td>0.482121</td>\n      <td>0.313659</td>\n      <td>0.232441</td>\n      <td>0.482121</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.990200</td>\n      <td>0.985807</td>\n      <td>0.523633</td>\n      <td>0.430543</td>\n      <td>0.385916</td>\n      <td>0.523633</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nüìã Classification Report for microsoft/deberta-v3-base:\n              precision    recall  f1-score   support\n\n         neg       0.56      0.41      0.47       612\n     neutral       0.52      0.88      0.66      1175\n         pos       0.00      0.00      0.00       647\n\n    accuracy                           0.53      2434\n   macro avg       0.36      0.43      0.38      2434\nweighted avg       0.39      0.53      0.44      2434\n\n\nüíæ Saved results ‚Üí deberta_telugu_results.pkl\n‚úÖ Final: F1=0.4354, Accuracy=0.5288\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"\"\"\"\nüéØ TELUGU SENTIMENT ANALYSIS ‚Äî SINGLE MODEL VERSION\nModel: google/muril-base-cased\n\n‚úÖ Uses pre-split train.csv, val.csv, test.csv\n‚úÖ Generates classification report + confusion matrix\n‚úÖ Saves results in a .pkl file\n\"\"\"\n\n# =============================================================================\n# IMPORTS\n# =============================================================================\nimport os, gc, time, warnings, pickle\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nfrom torch.utils.data import Dataset\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n\nwarnings.filterwarnings('ignore')\n\n# =============================================================================\n# CONFIG\n# =============================================================================\nMODEL_NAME = \"google/muril-base-cased\"\nMODEL_DESC = \"MuRIL - Multilingual BERT trained on Indian languages\"\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nCONFIG = {\n    'device_type': 'GPU' if torch.cuda.is_available() else 'CPU',\n    'device': DEVICE,\n    'batch_size': 16,\n    'max_length': 192,\n    'learning_rate': 2e-5,\n    'num_epochs': 3,\n    'mixed_precision': 'fp16' if torch.cuda.is_available() else None\n}\n\nprint(f\"‚úÖ Using device: {CONFIG['device_type']}\")\nprint(f\"üöÄ Model: {MODEL_NAME} ‚Äî {MODEL_DESC}\")\n\n# =============================================================================\n# DATA PROCESSOR (PRE-SPLIT)\n# =============================================================================\nclass TeluguSentimentProcessor:\n    def __init__(self, train_path, val_path, test_path):\n        self.train_path = train_path\n        self.val_path = val_path\n        self.test_path = test_path\n        self.label_encoder = None\n        self.data_splits = {}\n        self.task_info = {}\n\n    def load_and_process_dataset(self):\n        def load_file(path):\n            for enc in ['utf-8', 'utf-8-sig', 'latin1']:\n                try:\n                    return pd.read_csv(path, encoding=enc)\n                except:\n                    continue\n            raise ValueError(f\"‚ùå Could not load {path}\")\n\n        print(\"\\nüìä Loading Telugu Sentiment Data (Pre-split)...\")\n        df_train = load_file(self.train_path)\n        df_val = load_file(self.val_path)\n        df_test = load_file(self.test_path)\n\n        # Detect text/label columns\n        text_col = next((c for c in ['Text', 'Sentence', 'text', 'sentence', 'content'] if c in df_train.columns), df_train.columns[0])\n        label_col = next((c for c in ['Sentiment', 'label', 'Label', 'sentiment'] if c in df_train.columns), df_train.columns[-1])\n        print(f\"‚úÖ Text column: {text_col}, Label column: {label_col}\")\n\n        # Combine all for encoding\n        all_data = pd.concat([df_train, df_val, df_test])\n        self.label_encoder = LabelEncoder()\n        self.label_encoder.fit(all_data[label_col].astype(str))\n\n        # Encode labels\n        for df in [df_train, df_val, df_test]:\n            df['sentiment_encoded'] = self.label_encoder.transform(df[label_col].astype(str))\n\n        # Store splits\n        self.data_splits = {\n            'train': {'texts': df_train[text_col].values, 'labels': df_train['sentiment_encoded'].values},\n            'val': {'texts': df_val[text_col].values, 'labels': df_val['sentiment_encoded'].values},\n            'test': {'texts': df_test[text_col].values, 'labels': df_test['sentiment_encoded'].values}\n        }\n\n        self.task_info = {\n            'num_labels': len(self.label_encoder.classes_),\n            'labels': list(self.label_encoder.classes_),\n            'text_column': text_col,\n            'sentiment_column': label_col\n        }\n\n        print(f\"üìä Train={len(df_train)}, Val={len(df_val)}, Test={len(df_test)}\")\n        print(f\"üè∑Ô∏è Classes: {list(self.label_encoder.classes_)}\")\n\n        return self.task_info\n\n# =============================================================================\n# DATASET\n# =============================================================================\nclass TeluguSentimentDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self): return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoding = self.tokenizer(\n            str(self.texts[idx]),\n            truncation=True, padding='max_length',\n            max_length=self.max_length, return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(int(self.labels[idx]), dtype=torch.long)\n        }\n\n# =============================================================================\n# METRICS\n# =============================================================================\ndef compute_metrics(eval_pred):\n    preds, labels = eval_pred\n    preds = np.argmax(preds, axis=1)\n    acc = accuracy_score(labels, preds)\n    prec, rec, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n    return {'accuracy': acc, 'f1': f1, 'precision': prec, 'recall': rec}\n\ndef evaluate_and_plot(model_name, labels, preds, label_encoder):\n    \"\"\"Generate classification report + confusion matrix\"\"\"\n    report = classification_report(labels, preds, target_names=label_encoder.classes_)\n    print(f\"\\nüìã Classification Report for {model_name}:\\n{report}\")\n\n    cm = confusion_matrix(labels, preds)\n    plt.figure(figsize=(5, 4))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=label_encoder.classes_,\n                yticklabels=label_encoder.classes_)\n    plt.title(f\"Confusion Matrix - {model_name}\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.tight_layout()\n    img_name = f\"{model_name.replace('/', '_')}_confusion_matrix.png\"\n    plt.savefig(img_name)\n    plt.close()\n\n    # Save report as pickle\n    with open(f\"{model_name.replace('/', '_')}_classification_report.pkl\", \"wb\") as f:\n        pickle.dump({'report': report, 'confusion_matrix': cm}, f)\n\n# =============================================================================\n# TRAINING FUNCTION\n# =============================================================================\ndef train_muril(processor):\n    print(f\"\\nüöÄ Training single model: {MODEL_NAME}\")\n    splits = processor.data_splits\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        MODEL_NAME,\n        num_labels=processor.task_info['num_labels']\n    ).to(DEVICE)\n\n    train_ds = TeluguSentimentDataset(splits['train']['texts'], splits['train']['labels'], tokenizer, CONFIG['max_length'])\n    val_ds = TeluguSentimentDataset(splits['val']['texts'], splits['val']['labels'], tokenizer, CONFIG['max_length'])\n    test_ds = TeluguSentimentDataset(splits['test']['texts'], splits['test']['labels'], tokenizer, CONFIG['max_length'])\n\n    args = TrainingArguments(\n        output_dir=f\"./{MODEL_NAME.replace('/', '_')}\",\n        num_train_epochs=CONFIG['num_epochs'],\n        per_device_train_batch_size=CONFIG['batch_size'],\n        per_device_eval_batch_size=CONFIG['batch_size'],\n        learning_rate=CONFIG['learning_rate'],\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        logging_steps=100,\n        fp16=(CONFIG['mixed_precision'] == 'fp16'),\n        report_to=[]\n    )\n\n    trainer = Trainer(\n        model=model, args=args,\n        train_dataset=train_ds, eval_dataset=val_ds,\n        compute_metrics=compute_metrics\n    )\n\n    trainer.train()\n    eval_result = trainer.evaluate(test_ds)\n    preds = np.argmax(trainer.predict(test_ds).predictions, axis=1)\n    labels = splits['test']['labels']\n\n    evaluate_and_plot(MODEL_NAME, labels, preds, processor.label_encoder)\n\n    result = {\n        'model_name': MODEL_NAME,\n        'description': MODEL_DESC,\n        'f1': eval_result['eval_f1'],\n        'accuracy': eval_result['eval_accuracy'],\n        'precision': eval_result['eval_precision'],\n        'recall': eval_result['eval_recall']\n    }\n\n    # Save PKL\n    with open(\"muril_telugu_results.pkl\", \"wb\") as f:\n        pickle.dump({\n            'results': result,\n            'labels': processor.label_encoder.classes_,\n            'report_file': f\"{MODEL_NAME.replace('/', '_')}_classification_report.pkl\",\n            'confusion_matrix_image': f\"{MODEL_NAME.replace('/', '_')}_confusion_matrix.png\"\n        }, f)\n\n    print(f\"\\nüíæ Saved results ‚Üí muril_telugu_results.pkl\")\n    print(f\"‚úÖ Final: F1={result['f1']:.4f}, Accuracy={result['accuracy']:.4f}\")\n\n# =============================================================================\n# MAIN\n# =============================================================================\ndef main():\n    processor = TeluguSentimentProcessor(\n        train_path=\"/kaggle/input/nlpdataset/train.csv\",\n        val_path=\"/kaggle/input/nlpdataset/val.csv\",\n        test_path=\"/kaggle/input/nlpdataset/test.csv\"\n    )\n    processor.load_and_process_dataset()\n    train_muril(processor)\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T17:16:55.645162Z","iopub.execute_input":"2025-11-07T17:16:55.645874Z","iopub.status.idle":"2025-11-07T17:33:56.067595Z","shell.execute_reply.started":"2025-11-07T17:16:55.645846Z","shell.execute_reply":"2025-11-07T17:33:56.066469Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Using device: GPU\nüöÄ Model: google/muril-base-cased ‚Äî MuRIL - Multilingual BERT trained on Indian languages\n\nüìä Loading Telugu Sentiment Data (Pre-split)...\n‚úÖ Text column: Sentence, Label column: Sentiment\nüìä Train=19464, Val=2433, Test=2434\nüè∑Ô∏è Classes: ['neg', 'neutral', 'pos']\n\nüöÄ Training single model: google/muril-base-cased\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1219' max='1827' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1219/1827 16:29 < 08:14, 1.23 it/s, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.815400</td>\n      <td>0.772269</td>\n      <td>0.726264</td>\n      <td>0.720081</td>\n      <td>0.728362</td>\n      <td>0.726264</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.677600</td>\n      <td>0.675668</td>\n      <td>0.732018</td>\n      <td>0.731165</td>\n      <td>0.730800</td>\n      <td>0.732018</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m             _save(\n\u001b[0m\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:815] . PytorchStreamWriter failed writing file data/1: file write failed","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/4228474309.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_37/4228474309.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    234\u001b[0m     )\n\u001b[1;32m    235\u001b[0m     \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_and_process_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0mtrain_muril\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/4228474309.py\u001b[0m in \u001b[0;36mtrain_muril\u001b[0;34m(processor)\u001b[0m\n\u001b[1;32m    196\u001b[0m     )\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0meval_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2204\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2206\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2207\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m             self._maybe_log_save_evaluate(\n\u001b[0m\u001b[1;32m   2658\u001b[0m                 \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[1;32m   3101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3103\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3104\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[0;34m(self, model, trial)\u001b[0m\n\u001b[1;32m   3209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_only_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3210\u001b[0m             \u001b[0;31m# Save optimizer and scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3211\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_optimizer_and_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3212\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_scaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3213\u001b[0m             \u001b[0;31m# Save RNG state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_optimizer_and_scheduler\u001b[0;34m(self, output_dir)\u001b[0m\n\u001b[1;32m   3336\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3337\u001b[0m             \u001b[0;31m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3338\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPTIMIZER_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3340\u001b[0m         \u001b[0;31m# Save SCHEDULER & SCALER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m             _save(\n\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:626] . unexpected pos 52032 vs 51924"],"ename":"RuntimeError","evalue":"[enforce fail at inline_container.cc:626] . unexpected pos 52032 vs 51924","output_type":"error"}],"execution_count":6}]}