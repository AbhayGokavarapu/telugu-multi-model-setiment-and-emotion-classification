{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================\n# âœ… NLP Project: XLM-RoBERTa-Base Emotion Classification (Weighted Metrics)\n# =============================\n\n!pip install -q transformers datasets scikit-learn torch\n\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import (\n    confusion_matrix,\n    classification_report,\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    ConfusionMatrixDisplay\n)\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import Dataset\nimport pickle\nimport matplotlib.pyplot as plt\n\n# -----------------------------\n# 1. Load Dataset\n# -----------------------------\ntrain_df = pd.read_csv(\"/kaggle/input/nlpdataset/train.csv\")[['Sentence', 'Emotion']]\nval_df = pd.read_csv(\"/kaggle/input/nlpdataset/val.csv\")[['Sentence', 'Emotion']]\ntest_df = pd.read_csv(\"/kaggle/input/nlpdataset/test.csv\")[['Sentence', 'Emotion']]\n\n# Encode labels\nle = LabelEncoder()\ntrain_df['Emotion'] = le.fit_transform(train_df['Emotion'])\nval_df['Emotion'] = le.transform(val_df['Emotion'])\ntest_df['Emotion'] = le.transform(test_df['Emotion'])\nnum_labels = len(le.classes_)\n\n# Convert to HuggingFace Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\n# -----------------------------\n# 2. Tokenization\n# -----------------------------\ntokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"Sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\nval_dataset = val_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\n\n# Rename label column\ntrain_dataset = train_dataset.rename_column(\"Emotion\", \"labels\")\nval_dataset = val_dataset.rename_column(\"Emotion\", \"labels\")\ntest_dataset = test_dataset.rename_column(\"Emotion\", \"labels\")\n\n# Set PyTorch format\ncolumns = [\"input_ids\", \"attention_mask\", \"labels\"]\ntrain_dataset.set_format(\"torch\", columns=columns)\nval_dataset.set_format(\"torch\", columns=columns)\ntest_dataset.set_format(\"torch\", columns=columns)\n\n# -----------------------------\n# 3. Model\n# -----------------------------\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"xlm-roberta-base\", \n    num_labels=num_labels\n)\n\n# -----------------------------\n# 4. Training Arguments\n# -----------------------------\ntraining_args = TrainingArguments(\n    output_dir=\"./xlm_base_results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"no\",\n    logging_strategy=\"steps\",\n    logging_steps=50,\n    report_to=\"none\",\n    load_best_model_at_end=False\n)\n\n# Metrics function\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    acc = accuracy_score(labels, preds)\n    return {\"accuracy\": acc}\n\n# -----------------------------\n# 5. Trainer\n# -----------------------------\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics\n)\n\n# -----------------------------\n# 6. Train\n# -----------------------------\ntrainer.train()\n\n# -----------------------------\n# 7. Predict on Test Set\n# -----------------------------\npreds_output = trainer.predict(test_dataset)\ny_pred = np.argmax(preds_output.predictions, axis=-1)\ny_true = test_df[\"Emotion\"].values\nclasses = le.classes_\n\n# -----------------------------\n# 8. Weighted Metrics\n# -----------------------------\nweighted_precision = precision_score(y_true, y_pred, average=\"weighted\")\nweighted_recall = recall_score(y_true, y_pred, average=\"weighted\")\nweighted_f1 = f1_score(y_true, y_pred, average=\"weighted\")\naccuracy = accuracy_score(y_true, y_pred)\n\nprint(\"\\nðŸ“Š Weighted Metrics for XLM-RoBERTa-Base\")\nprint(\"-------------------------------------------\")\nprint(f\"Accuracy          : {accuracy:.4f}\")\nprint(f\"Weighted Precision: {weighted_precision:.4f}\")\nprint(f\"Weighted Recall   : {weighted_recall:.4f}\")\nprint(f\"Weighted F1-score : {weighted_f1:.4f}\")\n\n# Confusion Matrix\ncm = confusion_matrix(y_true, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\ndisp.plot(cmap=\"Purples\", values_format=\"d\")\nplt.title(\"XLM-RoBERTa-Base - Weighted Confusion Matrix\")\nplt.show()\n\n# Classification Report\nreport = classification_report(y_true, y_pred, target_names=classes, digits=3)\nprint(\"\\nDetailed Classification Report:\")\nprint(report)\n\n# -----------------------------\n# 9. Save Results to Pickle\n# -----------------------------\nresults = {\n    \"model_name\": \"xlm-roberta-base\",\n    \"label_encoder\": le,\n    \"preds\": y_pred,\n    \"confusion_matrix\": cm,\n    \"classification_report\": report,\n    \"accuracy\": accuracy,\n    \"weighted_precision\": weighted_precision,\n    \"weighted_recall\": weighted_recall,\n    \"weighted_f1\": weighted_f1\n}\n\nwith open(\"xlm_base_weighted_results.pkl\", \"wb\") as f:\n    pickle.dump(results, f)\n\nprint(\"\\nâœ… All results saved to xlm_base_weighted_results.pkl\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}